# ğŸ“Œ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install transformers tqdm scikit-learn

# ğŸ“Œ 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from transformers import AutoTokenizer, AutoModel
import torch

# ğŸ“Œ 3. GPU ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ğŸ“Œ 4. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")  # title, paragraph_index, paragraph ìˆìŒ

# ğŸ“Œ 5. ë¬¸ë‹¨ ë¶„ë¦¬ í•¨ìˆ˜
def split_into_paragraphs(text):
    paragraphs = [p.strip() for p in text.split('\n') if len(p.strip()) > 10]
    return paragraphs

train_paragraphs = []
train_labels = []

for _, row in tqdm(train_df.iterrows(), total=len(train_df)):
    paragraphs = split_into_paragraphs(row['full_text'])
    for p in paragraphs:
        train_paragraphs.append(p)
        train_labels.append(row['generated'])  # ì „ì²´ ê¸€ì˜ ë¼ë²¨ì„ ë¬¸ë‹¨ì—ë„ ë™ì¼í•˜ê²Œ ë¶€ì—¬

# ğŸ“Œ 6. Tokenizer ë° ëª¨ë¸ ë¡œë“œ (KLUE RoBERTa)
model_name = "klue/roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).to(device)
model.eval()

# ğŸ“Œ 7. ë¬¸ë‹¨ ì„ë² ë”© í•¨ìˆ˜ (CLS í† í° ì‚¬ìš©)
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()  # CLS token

# ğŸ“Œ 8. ë¬¸ë‹¨ ì„ë² ë”© ì¶”ì¶œ
train_embeddings = []
for text in tqdm(train_paragraphs):
    emb = get_embedding(text)
    train_embeddings.append(emb)

X = np.array(train_embeddings)
y = np.array(train_labels)

# ğŸ“Œ 9. ë¶„ë¥˜ê¸° í•™ìŠµ
clf = LogisticRegression(max_iter=1000)
clf.fit(X, y)

# ğŸ“Œ 10. í…ŒìŠ¤íŠ¸ ë¬¸ë‹¨ ì„ë² ë”©
test_paragraphs = test_df['paragraph'].tolist()
test_embeddings = [get_embedding(p) for p in tqdm(test_paragraphs)]
X_test = np.array(test_embeddings)

# ğŸ“Œ 11. ì˜ˆì¸¡ ë° ì €ì¥
probs = clf.predict_proba(X_test)[:, 1]  # AIì¼ í™•ë¥ 

submission = pd.read_csv("sample_submission.csv")
submission['prob'] = probs
submission.to_csv("submission.csv", index=False)

print("âœ… submission.csv ì €ì¥ ì™„ë£Œ")
