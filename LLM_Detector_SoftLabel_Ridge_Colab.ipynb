{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Soft Labeling + Ridge \ubaa8\ub378 (Colab \uc804\uc6a9)\n\ud55c\uad6d\uc5b4 LLM \ud310\ubcc4 \ub300\ud68c\uc6a9"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 1. \ub77c\uc774\ube0c\ub7ec\ub9ac \uc124\uce58\n", "!pip install transformers scikit-learn tqdm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 2. \ub4dc\ub77c\uc774\ube0c \ub9c8\uc6b4\ud2b8\n", "from google.colab import drive\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 3. \ub77c\uc774\ube0c\ub7ec\ub9ac \ubd88\ub7ec\uc624\uae30\n", "import pandas as pd\n", "import numpy as np\n", "from tqdm import tqdm\n", "from sklearn.linear_model import Ridge\n", "from transformers import AutoTokenizer, AutoModel\n", "import torch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 4. \ub370\uc774\ud130 \uacbd\ub85c \uc124\uc815 (\ub0b4 \ub4dc\ub77c\uc774\ube0c \uacbd\ub85c\uc5d0 \ub9de\uac8c \uc218\uc815)\n", "train_path = \"/content/drive/MyDrive/Colab Notebooks/train.csv\"\n", "test_path = \"/content/drive/MyDrive/Colab Notebooks/test.csv\"\n", "sample_path = \"/content/drive/MyDrive/Colab Notebooks/sample_submission.csv\"\n", "\n", "train_df = pd.read_csv(train_path)\n", "test_df = pd.read_csv(test_path)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 5. \ubb38\ub2e8 \ubd84\ub9ac + soft label \uc0dd\uc131\n", "def split_into_paragraphs(text):\n", "    return [p.strip() for p in text.split('\\n') if len(p.strip()) > 10]\n", "\n", "train_paragraphs = []\n", "soft_labels = []\n", "for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n", "    paragraphs = split_into_paragraphs(row['full_text'])\n", "    label = row['generated']\n", "    soft_label = label / len(paragraphs) if len(paragraphs) > 0 else 0.0\n", "    for p in paragraphs:\n", "        train_paragraphs.append(p)\n", "        soft_labels.append(soft_label)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 6. BERT \ub85c\ub4dc + \ud3c9\uade0 \uc784\ubca0\ub529\n", "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n", "model = AutoModel.from_pretrained(\"klue/roberta-base\").to(device).eval()\n", "\n", "def get_avg_embedding(text_list, batch_size=16):\n", "    embeddings = []\n", "    for i in tqdm(range(0, len(text_list), batch_size)):\n", "        batch = text_list[i:i+batch_size]\n", "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n", "        inputs = {k: v.to(device) for k, v in inputs.items()}\n", "        with torch.no_grad():\n", "            outputs = model(**inputs)\n", "        avg_pool = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n", "        embeddings.extend(avg_pool)\n", "    return embeddings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 7. \ud559\uc2b5 \ubc0f \uc608\uce21\n", "X = np.array(get_avg_embedding(train_paragraphs))\n", "y = np.array(soft_labels)\n", "\n", "clf = Ridge(alpha=1.0)\n", "clf.fit(X, y)\n", "\n", "X_test = np.array(get_avg_embedding(test_df['paragraph_text'].tolist()))\n", "probs = np.clip(clf.predict(X_test), 0, 1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udccc 8. \uc81c\ucd9c \ud30c\uc77c \uc800\uc7a5\n", "submission = pd.read_csv(sample_path)\n", "submission['generated'] = probs\n", "submission.to_csv(\"/content/submission.csv\", index=False)\n", "print(\"\u2705 \uc81c\ucd9c \ud30c\uc77c \uc800\uc7a5 \uc644\ub8cc: /content/submission.csv\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 2}